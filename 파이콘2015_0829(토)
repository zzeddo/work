테마 : 파이썬 다같이 , 성공적.
행사 : 파이콘 한국(PYCON KOREA) 2015
일시 : 2015년 8월 29일(토)~30일(일)
장소 : 상암동 누리꿈 스퀘어 비지니스몰 3층/4층
파이콘 한국 홈페이지 : http://pycon.kr
주체: 사단법인 파이썬사용자모임
협찬 : nipa(정보통신산업진흥원)
후원 : 마이크로소프트, Google Developers, DEVSISTERS, spoqa, The betapacking company
---------------------------------------
첫째날

[Opening] - 09:50~10:00
5분 정도 지나 9시 50분에 시작함.
메르스로 인하여 한차례 연기됨.
PYCON에 참석하지 않은 사람은 있어도 한번만 참가한 사람은 없다.
1. Pycon 소개
 - 전세계 38개 지역에서 개최
 - "Everybody Pays" - 참가자, 자원 봉사자, 발표자 모두 돈을 내서 참석하여 오프소스 활동에 재정 지원
 - Tutorial/Conference/Sprints/Summit
2. Pycon KR 소개
 - 2014년에 처음 시작(17세션/450명)
 - 2015년 33개 세션/700명
 - 2016년 PyCon APAC 서울 개최 확정!
3. 부탁의 말씀
 - 다양성을 존중하고 서로의 의견을 경청부탁드립니다.
4. Sponsers
 - Microsoft, Google, Spoda, nipa(정보통신산업진흥원), SKplanet....

------------------------------------------
[Keynote - 파이썬 : 내다보기] - 10:00~10:20
발표자 : 서상현
파이썬을 사용한지 12년 정도 되었으며, iron-python 개발에 참여 하였음
컴퓨터 언어란 사람이 컴퓨터와 서로 대화하는데 필요한 언어이다.
   - 표준국어대사전. 대화하다 02

사람이 컴퓨터와, 사람이 사람과
네가 의도한뜻, 네가 이해한 뜻

사과를 주세요하면 사과가 아닌 배를 줄수도 있다.(의도한 바와 다르다)
하지만 풋사과나 내가 생각한 사과와 다른 사과를 줄 수 있다.(원하는 것 이상의 결과)

프로그램 언어에는
사전에는 임의의 고정된(arbitary fixed) 순서가 있어야 한다.(예 : Pytyon Dictionary, Vector,List 등)

사람과 사람이 소통하기도 힘든데, 사람과 컴퓨터가 소통하기 위한 프로그램 언어를 만드는 것은 어려운 일이다.

Python 3.5(2015년 9월 출시)에 PEP 484:Type Hints가 들어가서 명시적으로 형(type) 정의 가능해 진다.
(기존 Python은 실행시 변수에 대입되는 값에 따라 동적으로 형(type)을 정의한다.)

특정시스템에 접속한 사용자ID와 접속일을 통한 사용 패턴을 분석한는 프로그램을 만든다고 하자.
- Cohorts 분석을 python 3.4와 3.5 버젼간의 차이를 가지고 설명함

명시적인 형(type)에서 
목록(list)이 비어 있으면?
목록이 정렬되어 있지 않으면?

python은 많은 사람들이 사용하고 기능 개발에 참여하면서 대중적으로 인기 있는 개발 언어가 되었다.

------------------------------------
[Docker] - 11:00~12:00
발표자 : 안병욱
제목 : Docker를 이용하여 Python 개발 환경을 빠르게 구성하고, 백앤드 서비스를 탐색하는 기술, 실행환경

주말인데도 불구하고 참석해 주셔서 감사합니다.
발표 주제는 Docker, Python, 개발환경, 백엔드 서비스, 실행환경임
유사한 주제로 많은 PYCON에서도 다루고 있다.

발표자는 디바이스 드라이버(windows, linux) 개발 경험과 미디어 전송 경험을 기반으로 CDN 경험함
SKplanet에서 플랫폼 아키텍처 관련 업무를 수행하고 있습니다.

다양한 오픈소스 기술에 대하여 논하게 될 것이다.
Docker, YML, NGINX, MYSQL등

DOCKER
CONTAINER
DOCKER-MACHINE
DOCKER-COMPOSE
SAMPLE APP Architecture
TEST Environment
PERFORMANCE 주제로 발표를 하겠습니다.

Docker는 개발자와 시스템어드맨을 위한 분산 어플리케이션을 위한 오픈 플랫폼이다.
https://www.docker.com/BUILD, SHIP, RUN

1. Docker는 가상환경(기술)에 필수 요소로 되어 가고 있다.

Docker는 Container기반이며 VM과 차이는
VM은 Application 수행을 위하여 논리적으로 OS, Lib/Binary를 분리하여 제공하지만
Docker의 Container는 OS와 Lib/Binary를 공유하지만 Application을 논리적으로 분리한다.(자원 효율성 뛰어남)

2. Dockerfile을 통하여 python container 환경을 만들어 보자(ubuntu 환경)
$ cat dockerfile
FROM ubuntu
RUN\
  apt_get update && \
  apt-get install -y python python-dev python-pip python-virtualenv && \
  ..
$ docker build -t="mypython" image 생성(현재 디렉토리에 있는 Dockerfile 참조)
$ docker images - Docker Host의 image 확인
$ docker run -it mypython python - container 실행
$ docker ps -a - docker container 확인
$ top - 명령을 통하여 프로세스 상에 docker 확인 가능한다.(nginx)
  - docker container : ubuntu 2ea
  - container : ubuntu

3. Docker Machine
 - Docker Machine은 다양한 Application(Virtual Box, VMWare)을 Container를 실행하게 하는 역활 수행
 - DOCKER-COMPOSE는 자신이 만든 Container의 환경 설정(변수등)을 가능케 한다.
    $ cat docker-compose.xml
      db :
       image : mysql
       environment : .....
4. Sample Applicaiton
 - 3Tier(Presentation, Business, Data(DB)) 구성을 Docker로 해보자
 - Presentation : [앞단에 Load Balancer 사용] ANGULAR JSON
   Business : [앞단에 Load Balancer 사용] Python REST API
   DATA(DB) : DB

5. Test Environment
 - Traffic Gen. Server
 - Docker Host
   Presentation(WEB), Business(WAS), Data(DB) 구동됨

6. Test Case
  http://web.api:8000/shopping/APIs를 가진 예로

  web_api_case_01(Web 거침)
    Django(1.8.3)+djangodb+MySQL 5.6.23

  web_api_case_02(Web을 거치지 않음)
    Django(1.8.3)+djangodb+MySQL 5.6.23
  ...
  nginx : 1.9.2
  NGINX-GUNICORN(gunicorn docker_django.wsgi:application -w 12)

테스트를 하면서 Docker Host에 5개의 Container에서 1~2만 TPS 성능으로 높지 않았다.
(즉. 대용량 Transaction이 필요한 환경에서는 Docker를 통한 Container 분산하여 처리하기 보다는
 현재 WAS에서 JOB을 분산하여 처리하는 것이 적절한 것으로 생각)

7. DEMO
대용량 Transaction 처리를 위한 Frontend-Backend 구성을 바꿈
성능 점검은 nGrinder를 통하여 TPS 측정

즉 Docker의 용도는 Transaction 처리르 위한 분산환경 구성도 가능하지만
대용량 Transaction 처리를 목적으로 사용하기 보다는 사무환경(예 MS Office) 혹은 개발환경에 적합한 것으로 생각된다.
그러나 개발 환경에서 Docker를 구성하여 다양한 구성을 쉽게하여 다양한 조합으로 테스트 가능하다.
즉 기능 & 성능 검증시 Docker 환경으로 빠르게 아키텍처를 구성하고 테스트 수행하게 한다.

실제 업무에서는 기능 및 성능테스트 환경 구성에 많은 시간과 비용이 발생하지만
Docker를 개발 환경에 적용할 경우, 혁신적으로 적용 가능할 것으로 판단됨.

--------------------------------------------------------
[Writing the fast Code] - 12:00~13:00
발표자 : 김영근

프로파일링에 대하여 배워 가시면 좋겠습니다.
10만명이 사용하는 코드의 성능을 1초만 개선하면 10만초, 하루하고도 3시간 46분 절약가능함
(하지만 다른 곳에 낭비하겠지만)

컴퓨터 동작 원리
  - 입출력 장치 <<< 넘사벽 <<< 메모리
    모스(Morse) 부호 = 21bps(초당 3글짜 정도)
    Modem(2400) = 2400bps
    CDMA(2G) = 153kbit/s
    LTE = 100Mbit/s
    ....
    DDR3 1600Mhz = 12.8GB/s
    FSB 400(old XEON) = 12.8GB/s
    L2 Cache(i7-4790X) = 308GB/s
    * 메모리가 넘사벽으로 빠르다(DDR3 <<< L2 Cache)
컴퓨터 내부적으로는 0과 1만 구성되어 있다.
어셈블러 예시 : addi $r1, $r2
Clock(Hz)은 컴퓨터의 성능을 나타내는 수치를 보여 준다 (i5 3.3Ghz)
인스트럭션당 한개 이상에 클럭이 필요함-클럭이 높을 수록 높은 성능 예상 가능

컴퓨터의 구성요소의 처리 시간을 상대적으로 표현하면
  - L2 Cache Access : 3초
    SSD Access : 1일~3일
    HDD Access : 1달~12달
    ... 즉 구성 요소간의 병목을 해결하지 못하면 컴퓨터에서 최적의 성능을 보장하기 힘들다.
컴퓨터 아키텍처 구성시, 이와 같이 구성요소들의 처리 속도를 정량적으로 평가하지 않으면
먼 우주로(무지 많은 시간 소요)부터 데이터를 가져와서 처리(짦은 시간)할 수 있다.
분산컴퓨팅의 경우 네트워크까지 고려해야 해서, 구성요소간의 관계는 더 복잡해 진다.

1초에 찰싹한번 = 1Hz
그러니까 일을 덜하면 덜 아프다가 아니라 더 빠르게 해야 한다.

1. 동작(순서)에 대하여 알아 보자
dis(pip install dis) - disassembler(코드 최적화를 위해 사용하는 패키지)
  - Assebler 형태로 python 코드를 명령어와 인자(파라미터)로 구분하여 보여줌

2. 성능에 대하여 알아 보자
어떻게 동작하는지 오케이, 그럼 얼마나 걸리는지?(각종 프로파일러로 
 - timeit
   ipython에서는 %timeit
   shell에서는 python -m timeit -c .... 
 - cProfile
   오버헤드 좀 있으나 정보량도 많음
 - profiling
   실시간, 이흥섭님의 세션을 참조
간단한 예제를 통하여 profile을 통하여 성능 개선을 해보자.
파보나 치킨 : http://fibonachicken.herokuapp.com --> 피보나치 수열 계산

문제 1. 피보나치킨은 nth피보나치 숫자가 아니라 n-1번째 피보나치를 찾아야 함
        즉 입력이 nth 피보나치일때 n-1번째 피보나치를 반환하면 됨.
문제 2. 근데 입력이 피보나치 수가 아니면?
        배운자의 정리에 의하면 치킨도르.. 아니 제켄도르프 정리를 적용.
필요한 함수
- 일단 피보나치 구현 fib
- 피보나치 수가 아닌지도 알아야 하니까 is_fibonacci
- 이전 피보나치 수를 알아야 하닌까 prev_fibonacci

$python -m cProfile fibonachicken.py
성능과 관련된 각 요소(함수)에 대한 처리 시간을 확인할 수 있으며, 이를 통해 개선함

아리송한 분은 Office Hour로!
근데 사실 잘 만들어 놓은 거 쓰는게 짱.(dis, profile 하는 시간 절약)
예를 들면 pandas 라이브러리와 같이 사용자가 많은 경우
성능에 대하여 이슈를 제기하면 다수의 사람들이 참여하여 성능 개선함.
(시장의 원리를 통해 안정성 및 성능 확보,  Scout Out And better performance)
상용라이브러리(예 : SAS등 제공)에서 사용자가 성능 이슈 제기하는 것도, 개선하는 것도 어려움.
(성당의 논리, Keep Calm And bad performance)

정리하면, 컴퓨터의 Hz(인스트럭션 처리 기본 단위)와 구성요소의 처리 성능을 통하여
컴퓨터의 잠재적인 성능을 최고로 끌어올려 사용하시기 바랍니다.

--------------------------------------------------------
[추천시스템이 Word2Vec을 만났을때] - 13:00~14:00
발표자 : 최규민(goodvc78@gmail.com)

해리와 셀리가 만났을때를 배경화면으로 발표 주제와 영화의 이미지를 매핑하고 있다.

저(최규민)은 아프리카 TV에서 추천 프로그램을 선정하는 일을 하고 있다.

1. Word2Vec
Word2Vec은 2013년 구글에서 발표한 자연어 처리 머신러닝으로
Word를 100차원, 200차원의 한정된 Vector로 표현함
2003년 Bengio, 2013sus Mikolov(구글)의 차이는 정확도(2배)와 성능(7배)에서 차이가 난다.
NNLM(Neural net Language Model)은 주어진 문서에서 단어 w가 Context(c)의 단어 결합확률이 최대가 되도록 단어 w를 학습시킴
위의 알고리즘을 기사 리뷰를 통하여 word2vec를 통해 학습을 시킨다.
예) 영화 암살에 대한 리뷰를 학습시키면
    3차원 Vector 공간에서 단어들이 군집화 되는 현상을 확인할 수 있다.
    (최고의 연기자 이정제, 전지연 등)
좀더 많은 데이터를 통해 학습해 보자.
암살 영화 리뷰(60만건)에 대하여 학습을 시키면(100차원) 
트레이닝 결과 : 114,359(단어수), 100(차원)으로 결과가 나옴
이를 단어의 의미 "인물", "영화"로 해석을 하면
  - model.most_similar(positive=['이정재'] -> 관련된 배우가 나옴
  - model.most_similar(positive=['암살'] -> 연관되는 영화가 나옴
  * 즉 분석 결과를 통해 의미(Sementics)까지 확인할 수 있다.

word2Vec의 쓰면서 느낀점
 - 장점
   . 사용이 심플하다.(10줄이면 코딩끝)
   . 라벨링 되지 않은 데이터로(Supervised Learing)
   . 사용의 편리성에 비해 엄청난 정확도를 보여 준다.
 - 단점
   . 왜 이렇게 결과가 나오는지 명확한 해석이 어려움(머신러닝 대표 특징)
     사용자의 의도가 들어가기 위해서는 다양한 조정 과정 필요함
   . 의도적인 성능 향상을 위해서는 많은 삽질이 필요

2. 영화 추첨(별점 예측)
  - 나의 별점 예측은 어떻게 할까요?
    나와 유사한 영화 취향이 비슷한 사람들의 별점을 통하여 나의 별점을 예측한다.(베이지안 추론)
  - 구현 과정
    . Data Processing : 사전 데이터 확보
    . Neighbborhood : 유사도 측정
    . Prediction
    * 사전 데이텉를 위해 미네소타 대학의 GroupLens 연구실에서 수집 CF를 연구를 위한 영화 별점 데이터 사용
  - Data Processing
    [User * Movie matrix] 유저와 영화에 대한 2차원 매트릭스를 만들어 별점을 매핑함
    유저 : me, me+1(나와 유사한 정도), me+2, me+3....
    영화 : 겨울왕국, 터미네이터.....
    * 메트릭스능 사용자와 영화가 많아 질 수록 Sparsity가 높아 진다.
    * 이를 해결하기 위하여 PCA, SVD 사용함(Dimensionality Reductions)
  - Neghborhood(유사도 측정)
    Jaccard, Euclidean, Cosine, Corr를 사용하여 오차값(MAE : Mean Absolute Error)이 적은 것을 선택한다.
    결과는 Euclidean이 가장 오차가 적게 나온다. 
    1) 가장 유사한 유저 TopN 결과를 리턴
    2) 모든 유저와 유사도 측정(Brute Force)
       - Scalability 문제(성능은 유저수의 제곱이 된다.)
       - 클러스터링(군집화)를 통하여 해결함(K-Means, Hierarchical Clustering), ...
   - Prediction(예측)
     나와 유사한 유저의 별점을 더욱 더 신뢰하도록 별점 예측(가중치를 주어야 한다.)
    1) 최근접 유저의 유사도를 측정해서
      ....
  - 유저 1번의 별점 예측 결과(평가한 결과, 예측한 결과를 비교하면 상당 수준의 정확도를 확인 가능하다.)
  - User Based CF를 장단점은
    . 최소한의 기본 지식만으로 구현 가능하지만, 고차원 정밀도가 sparse하게 분포하는 문제가 있다.

Model Based를 통한 개선해 보자

3. 추천 시스템(Model)을 통한 Word2Vec를 사용해 보자
유사한 영화끼리 뭉치도록 Word2Vec를 활용해 보자
내가 별점을 준 영화와 유사한 영화를 추전해 주는 방식으로 모델을 통하여 word2vec 사용하면 결과는 어떻게 될까?
Movie2Vec을 사용한다.(관심 분류 영화 추천)

4. Movie2Vec
단어 -> 영화 ID
문장 -> 유저
우선 Visualization을 위해 2차원 150건 이상 별점을 받은 영화에 대하여 보여줌(군집(경향성)을 파악 가능)
추천 시스템을 만든다면 사용자별 경향성(군집)에 따라 추천을 하면 좋을 것 같다.
  별정데이터 : 2000만건
  Vectoer : 100차원
  영화 : 1300건에 대하여 학습시키는데 8분 소요됨
우선 내가 별점을 준 영화에 대하여, 사전에 준비된 데이터(학습)을 통하여 
영화 추천을 받는 형태로 진행한다.

Movie2Vec 결과와 Corpus 재구성후(3.0미만 별정은 제외)하면 이전의 결과의 크게 차이 나는 것을 알 수 있다.
Word2Vec 학습시 Corpus 구성 및 비교 테스트가 중요하고 지속적인 학습이 필요하다.

영화 추천 모델을 
2차원으로 x축은 나의 평점, y는 영화 유사도를 통해 추천하도록 할 수도 있고
3차원으로 인기도를 추가하거나
4차원으로 개봉일을 추가하여 즉 다양한 Feature(축)를 추가하여 추천의 랭킹 문제를 풀 수 있습니다.

넷플릭스이 마이크로 장르에 대하여 잠시 알아보자
76,894개 마이크로 장르가 있으며
예를 들면 8~10세를 위한 고양이가 나오는 폭력 스릴러,
악마 같은 아이가 나오는 컬트 공포 영화, 유럽배경의 60년대 영국 여자가 나오는 영화 등등...
다양한 장르에 대하여 개인화를 위한 세분화를 한다.(사람이..)

만들어 보자
1. 영화를 중심거리 기반으로 인접 영화끼리 군집화(K-Means)
2. 군집된 영화들의 Tag 생성
만들어진 군집을 통하여, 의미를 부여해 보면 마이크로 장르와 유사하게 분류 가능하다.
하지만 만들어진 군집에서 의미 부여가 어려운 것도 있다.
각 부류에 대하여 나의 취향을 매핑하여 영화 추천을 받도록 한다........

5. Q/A
추천 결과에 대한 검증은 예측(Prediction)이기 때문에 평균값(Means) 비교등을 사용한다.
Word2Vec을 통한 학습은 단어의 순서와 인접에 따라 학습하나 Movie2Vec은 다양한 조건을 통해 정확도 높여감.
머신러닝에서는 학습을 위하여 좋고 많은 데이터도 필요하지만 목적성에 맞게 학습 방향(조건)을 제공하여 최적화 해 주어야 한다.(결과에 대하여서도 후보정을 통해 해석을 다르게 할 수도 있다.)


--------------------------------------------------------
[파이썬 기반의 대규모 알고리즘 트레이딩 시스템] - 15:00~16:00
발표자 : 김도형

파이션 기반의 알고리즘 트레이딩 시스템으로 Large Scale을 표현하려하니 대규모라고 하였습니다.
전산을 전공은 아니며, 제어 전공하였으면 증권사에 입사해서 구조화 파생상품(ELS(DLS)) 업무를 수행함
이율이 정해진 것이 아니라, 수익율에 따라 이익을 분배하는 업무 수행함.
현재 증권사를 나와서 회사 수행

1. 알고리즘 트레이딩이란?
 - 이런 응용분야에서도 python을 사용하고 장단점과 사용 라이브러리에 대하여 참조하시기 바랍니다.
   왜 이것을 python으로 구현했는지 보시기 바랍니다,
 - 알고리즘 트레이딩 시스템 = 증권업무 자동화 시스템 <> 혼자서 돈을 버는 기계
   알고리즘 트레이딩으로 실제 돈을 벌 수 있는지는 발표 이후 생각해 보시기 바라지만 
   혼자서 돈을 버는 시스템은 아닙니다.
 - 설 사이드(파는 것) vs 바이 사이드(사는 것)
   . 알고리즘 트레이딩(Sell) : 시스템 트레이딩, 대량최적집행, HFT마켓 메이킹, 파생상품 해지
                      거래소로 주문을 냄
                      시스템 트레이딩은 주식을 언제 사고 팔지 사전에 정의 가능함(Buy 성격이 큼)
   . 퀀트 운용(Buy) : 퀀트 펀드, 전략 인덱스, 로보어드바이저
                      사람이 개입하나 개입하지 않도록 Rule을 통하여 자동화하면 전략 인덱스가 되고
                      개인에게 적용한 것이 로보어드바이저임
                      거래소로 주문을 내지 않음
 - 대량 최적 집행(Optimal Execution)
   . 포토폴리오의사 결정 -> 브로커에 주문 도착 -> 집행 시작 -> 집행 완료
     (펀드 매니저)          (브로커)   [Algo OMS]   --최적 분할 및 집행--
   알고리즘 트레이딩 시스템(Algo OMS)은 최적 집행(Sell)을 수행하도록 판매 시점과 량을 조절한다.
   최적화 요소는 시간에 따라 최저 주식 가격의 분포를 판단하여 수량을 결정해야 한다.(전략이 필요함)
   예전에 알고리즘 트레이딩 시스템이 없을 떄는 사람이 투입되어 한사람에 20종목씩 시황을 보면서 진행한다.
 - 대량 최적 매미 전략의 종류
  . 단순 주문 : Peg(호가 연속 추종), Hidden Sniper(대량 주문 은폐)
  . 대량 집행 : TWAP(시간 분할), Volume In-Line(거래량 참여율 목표)
  . VWAP 추종 : Static VMAP(거래량 곡선 모형), Smart VWAP(장중 참여율 자동 조정)
  . 시장 충격 : IS(시장 충격 최소화), MOC(종가 목표)
    국내에서는 TWAP을 많이 사용한다.
 - 그냥 한번에 주식 주문을 하지 왜 나누어서 하는지에 대하여 알기 위해서는 시장의 흐름을 알아야 한다.

2. 마켓 메이킹(Market Making)
 - 주식은 수요와 공급의 시장의 법칙을 따르기 때문에
   Sell 주문과 Buy 주문의 관계에 따라 주가(가격)이 결정된다.
   Sell 주문(1,100원) 100개, Buy 주문(1,050원) 100개가 있으면 중간에 조정하는 세력이 있게 된다.
 - 유동성 공급(LP : Liquidity Providing)
 - 매수/매도 양방향 지정가 주문
   . 지정가 주문(Limit Order) : FIFO 큐 형태로 처리되나 나머지가 있으면 시장가 주문으로 진행된다.
   . 시장가 주문(Market Order) : 큐에서 주문 체결(소모)
   . 이로 인해 시간에 따라 주식 가격의 변동이 생기게 된다.
   . 그래서 주식을 매도시 원하는 가격의 매수가 없으면 종가는 하락을 하게 된다.
   . 펀드 매니저가 주식 매도 및 매수를 잘못하게 되어 고객에게 손해를 끼치게 되면 배임협의로 고소당한다.
 - 시장 충격(지정가로 50만주 매수할때 매도하는 사람이 싹 빠짐 혹은 
             지정가로 50만주 매도할때 매수하는 사람 싹 빠짐(시장가로 조정대기를 기다림))
 - 매매가 원할하게 될려면 지정가로 원만하게 매도, 매수가 될어야 하지만
   시장가를 통해 비싸게 팔고 싸게 팔려고 하면서 종가가 요동을 치게 된다.(HFT : High Frequency Trading)
 - 수익 = 매두/매도 스프레드 * 거래량
 - 리스크 관리 : 포지션/변동성/추세에 따른 스프레드 조정
 - HFT : High Frequency Trading
 - 알고리즘 트레이딩에서는 HFT와 리스크 관리와 수익을 최대로 하면서 수행 필요함빠지기 전략)

3.알고리즘 트레이딩 시스템과 일반적인 업무 자동화 시스템과의 차이점
 - 핵심 비즈니스 로직(=전략)이 고정되어 있지 않음
   . 변화하는 시장 상황에 따라 지속적인 업그레이드 필요
   . 개별 사용자(트레이더)별로 비즈니스 로직을 별도 개발 및 적용
   . 사용 및 관리가 쉬운 스크립터 언어 선호함

 - 오작동 = "초대형" 손실 ----> 죽는다.
   . 한맥증권 : 2013-12-12 15분간 460억원 손실 파산
   . Knight Capital : 2012-08--01 45분간 4억 4천만 달러 손실. 파산
   . 다양한 테스트 환경 및 시나리오 요구 조건 필요
    * 손실이 있는 쪽이 있으면 수익을 보는 쪽이 있다.
 
 - 발표자가 만든 트레이딩 시스템은 python 기반으로 작성되어 있으며
   전략에 따라 분석과 실행을 같이 수행하도록 만들어져 있음

 - 분석-매매 단일 플랫폼
  . 훌륭한 시장 분석 없이 흉륭한 매매전략은 생기지 않는다.
  . 알고리즘 매매의 파워는 자동화/대량처리를 통한 비용 절감
  . 매매에 필요한 파라미터를 수동으로 결정하면
    자동화/대량처리 불가능
    입력 실수로 인한 사고 위험
  . 알고리즘 매매가 원래의 파워를 발휘하려면
    1. 대규모의 실시간 병렬 처리 시장 분석이 필요하다.

 - Why Python 
   . 분석-매매 단일 플랫폼 구현 가능 : python만 가능함
   . 전략(비지니스 로직) 코드 개발 및 관리
   . 다양한 컴포넌트 연결

 - 파이썬 라이브러리는 Anaconda 기반으로 pandas를 주로 사용함
   STATMODELS(통계 및 시계열 분석)
   TA-LIB(주식), SEABORN(시각화)

4. 시장 정보
 - 틱데이터(Tick Data)
   . 종목정보 등 실시간으로 받는 정보
   . 2015년 상반기 체결 틱 데이터가 한종목에 10만건씩 오고 있다.(2200개 종목)
 - 일종 데이터(Intraday Data)
 - 일간 데이터(Daily Data)

5. 시장정보 데이터베이스 & API
 - 틱데이터(Memory DB) : Redis
 - 일간 데이터 : Mogos
 - ipython을 통하여 UI를 개발하여 보여줌

6. Message
 - 현재 ZeroMQ, Formula 미들웨어 사용함

7. 이벤트 기반 전략
 - 이벤트 = 실시간 시장 및 주문 정보
 - 알고리즘 트레이딩 시스템에서 CEP(Complex Event Processing) 사용함

8. 틱데이터 기반 백테스트(Back Test)
 - 성능 평가 및 사고 예방을 위하여 다양한 테스트 수행함

9. 구현예 : PEG & Layering
 - Layering : 지정가 주문을 여러 호가에 분할
 - Pegging : 호가 변화하는 경우 최우선 호가로 추종
 * 정석에 따라 구현하면 매매 사고가 발생한다. 시장 충격에 대응 불가함(정상적인 상황이 아님)
   도메인 지식 없이 알고리즘 트레이딩하면 망한다.

10. 마치며
  - Python for finance에 대하여 번역을 하고 있으며 많은 관심 바랍니다.

--------------------------------------------------------
[알고리즘 트레이딩 토론] - 16:00~17:00
- 알고리즘 트레이딩에서 종목에 따라 파라미터를 조정하며, 수행하나 이상 현상에 빨리 반응 필요하다.
- [장기 변동성] 시장 변동성은 시간에 따라 변동되는 형태는 결과적으로 정규분포 형태가 되면 
  이익과 손실도 없다(장기적)
  [단기 변동성] 비이성적 시장 상황이 발생해야 돈을 번다.
- Execution 전략(8개)에서 최고는 정해져 있으나(수식으로 풀 수 있음) 실행 가능함으로 논하면
  좋은 모델과 나쁜 모델이 있음
- 시장이 계속 바뀌는 이유는 Trader가 있기 때문이다.
- HFT 수준을 50ms 수준으로 하려면 python으로 힘듬(Compile 가능한 언어로 대체 필요)
- 우리나라의 경우 증권거래소에서 제공하는 데이터 Traffic을 정해져 있기 때문에(가장 떨어지는 증권사 기준)
  데이터 처리에는 문제가 없다.
- CEP를 사용할 경우 시스템 시스템 부하를 많이 발생하고, 유연하지 못한다.(대부분 포기함)
- 테이블 단위로 압축하여 데이터를 저장함(HTF5 - Tree 형태로 저장하여 Query로 뽑아냄)
  Write-Once Read-Many 
- 안정성 확보는
  실시간 처리로 데이터 수집 처리 부분은 Pandas와 같은 라이브러리 사용하지 않기 때문에
  오류가 날 확률을 그만큼 줄어 든다.

--------------------------------------------------------
[Character Encoding] - 16:00~17:00

1. Charater Encoding
 - Charater Encoding은 ASCII, EBCDIC, EUC_KR, CP949, UTF-8, UTF-16, UTF-32가 있다.(모스 부호도 Encoding)
   * Unicord는 Encoding이 아니라 UTF-8, UTF-16, UTF-32 표현하기 위한 규약
 - 한글을 표현하기 위해서는 EUC_KR, CP949, UTF-8, UTF-16BE, UTF-16LE로 표현 가능하지만
   표현되는 형태는 다르다.
 - ASCII 확장이 UTF-8
 - Unicode는 전세계 문자, 기호를 Codepoint에 매칭(한글, 타이 문자)
   많이 쓰이는 이모티콘도 정의
   . Code Point : U+로 표현, ASCII 영역은 U+0000~U+007F로 표현됨
 - Unicode != UTF-8
   . UTF-8은 Unicode를 표현하기 위한 Encoding 방법임

2. UTF-8
 - 모든 Unicode Codepoint를 다룰 수 있다.

--------------------------------------------------------
[한국어와 NLTK, Gensim의 만남] - 17:00~18:00
부제 : 영화 리뷰를 컴퓨터가 이해할 수 있는 형식으로 표현해서 센티멘트 분석하기
발표자 : 박은정

오늘은 Text Mining에 대한 주제로 하겠습니다.
발표자는 서울대학교 데이터마이닝 센터 박사과정에 있으며, 팀포퐁 멤버, KoNLPy메인테이너

1. 발표에가 다루는 내용
 - 단어/문서를 컴퓨터가 이해할 수 있게 하는 표현 방법
 - KoNLPy, NLTK, Gensim의 간단한(?) 사용법
 . 이 발표를 득고 나면 센티멘트 분석을 할 수 있음
 - 제외 : 토픽모델링은 Gensim에 포함되어 있지만 일관된 흐름을 위해 제외
 - 기타 사항 : Python 3 기준

2. 역사
 - 1997년 지금으로부터 20년전에 IBM 딥블루가 체스세계챔피온(가리 카스파로프)를 이기다.
 - 컴퓨터가 더 똑똑해지기 위해서는
   정형화된 규칙이 없는 지식을 컴퓨터에게 어떻게 전달할 것인지 알아야 한다.
   -- Bengio et al., Deep Learing(2015, MIT Press)

3. 텍스트를 "표현"하는 다양한 방법
 - 먼저, 단어를 표현하기 가장 쉬운 방법은 이진(Binary) 표현법
 - 어떻게 하면 구조가 없는 데이터인 텍스트의 의미를 컴퓨터가 잘 이해라 수 있을까?
   (Encoding은 이미 오래전부터 했지만 그것은 의미가 아님)
 - 텍스트의 의미를 이해한다는 유사한 의미를 가진 텍스트끼리 묶을 수 있다 
   "{단어, 문서} 포함 텍스트"
   one-hot vector라고 부르고 있다.
   하지만 이진 표현법을 사용하면 단어 간 유사도를 정의할 수 없다.
 - 같은 철학으로 문서를 표현한 것이, bag of words(BOW)
   . 단어가 문서에 존재한다/안한다(term existance)
   . 단어가 거리(term distance)
   . 그런데 공간의 차원이 너무 커서 문서간 유사도 지표의 효과가 떨어진다.(별 효과가 없다.)
     공간의 차원이 커질수록 데이터의 대부분은 공간의 꼭지에 분포하게 된다.
     문서간 거리의 편차가 매우 적어지게 됨.
     문서간 유사도가 "또이또이"
 - "그럼 워드넷 같은 텍소노미를 활용하는건 어때요?"
   . 모든 용어를 포함하려고는 하지만, 전문 도메인 용어는 빠져 있다.
 - "친구를 보면 그사람을 안다"을 통하여 "단어의 주변을 보면 그 단어를 안다"로 새로운 돌파구 마련
   . 주변부 단어의 맥락을 보고 단어의 의미를 추측 가능함(다른 나라 언어 공부할때 많이 활용)
   . 다시 말해 단어의 의미는 해당 단어의 문맥(Context)이 담고 있다.
     예) 영희가 철수에게 미안하다고 사과하면서 나무에서 갓 딴 맛있는 사과를 주었습니다.
         "사과"가 다른 의미로 사용된 것을 주변 문맥(Context)를 통하여 알수 있음

 - Co-occurrence를 정의하는 두가지 방법
   . Term-document matrix
   . Term-term matrix를 통하여 단어 간 유사도를 구할 수는 있다.
   . 하지만, 값들이 너무 skewed 되어 있고(즉, 빈도 높은 단어와 낮은 단어의 격차가 큼)
 - Neural embeddings
   . 아이디어 : 문맥에 있는 단어를 예측하라!
     언어 모델(Language model) 활용
   . 대표적인 사례 : word2vec*(2013년에 발표) 
 - [숙제] 북한 신년사를 word2vec을 통하여 2차원으로 분석해 보기  
   . 군집을 통하여 의미(의도)를 파악할 수 있다.
 - doc2vec*도 word2vec과 마찬가지로 활용할 수 있다.

3. python을 통한 실습
 - doc2vec를 통하여 문서를 읽어 들어 단어를 vector로 수치화(나중에 gensim으로 대체)
 - 단어와 문맥을 이용하여 단어를 표현할 수 있다.
   . 단어는 word2vec
   . 문서는 doc2vec를 통하여 분석한다.

4. 데이터는 어디서 구하지?
 - 한국어 영어 리뷰 토이 데이터 공개
   .Naver sentiment movie corpus v1.0
    Maas et al., 2011데이터셋 참조
    총 20만개 리뷰(수집된 64만개 중 샘플링)
    (긍정/부정 리뷰의 비율을 동일하게 샘플링, 중립을 포함하지 않음) 
    * http://github.com/ --> 꼭 확인하기

5. 자. 그럼 시작
1) KoNLPy로 데이터 전처리
   . Data preprocessing(feat. KoNLPy)
     데이터 읽기 -> 검증하기(컬럼의 수로 확인) -> 형태소로 토크나이징(Twitter 형태소 분석기 사용)
     질문> 형태소로 꼭 나눠야 하나요 -> 선택의 문제(대규모 분석에는 효과)
     질문> 품사(POS) 태그를 부탁해야 하나요? -> 선택의 문제(포함시 동의이음어 구별 가능함)
2) NLTK로 데이터 탐색
   . nltk.Text()를 사용하여 Exploration 수행함
   . text.plot(50) 상위 50개 단어에 대하여 분포 확인 가능(글자 깨지면 폰트 바꾸기)
   . Collocations(연어) : 인접하게 빈번하게 등장하는 단어(ex : "text" + "mining")

3) Sentiment classification with term existance 
   (Bag-of-words(term existence)로 문서 표현하고 분류)
  분류시 다양한 방법이 제공됨(예 : Naive Bayes classifer)

4) Sentiment classification with doc2vec(feat. Gensim)
 - Gensim을 만드신 분이 거제도에 살고 계심(PYCON 행사에 오지 못하심)
   Gensim은 사전(Dictionary)을 만드는 과정이 들어 가야 한다.

6. 마무리
 - 텍스트의 다양한 표현법에 대해 살펴 보았다.
 - 나는 어떤 테스크에 적용해볼 수 있을지 생각해 보자.
   예) 국회 의안 통과/폐기 예측
       주가 상승/하락 예측
       설비 로그를 통해 정상/오류 예측

감사합니다.
----
http://lucypark.kr
@echojuliett

--------------------------------------------------------
명패는 꼭 가져 오시기 바라며, 파이참 경품 10명에게 있습니다.

[Lighting Talk] - 18:00~19:00
5분간 말하는 거.
작년에는 5명의 Lighting Talk를 진행함
오늘의 규칙은 5분 이야기 하고 징이 울리고 30초 지나면 내려오게 함. 박수치지 말기

[첫번째 - 프로그래머가 이사하는 법] 변수민
1. 이사갈 집을 고르는 조건으로 대중교통의 접근성에 대한 평가에 대한 주제로 이야기 하겠습니다.
 - 자주 가는 지역과
 - 이사한 집과 자주 가는 지역간의 거리를 분석해 보자

2. 처음 생각한 생각한 것은 지도를 통한 Heat Map 생각했으나
   기능 축소하여 버스나 지하철을 갈아타지 않고 갈 수 있는 곳을 이사할 집으로 정함

3. 구조는 
   서울시교통정보시스템 -> REDIS -> Flask(AWS Beanstalk) 사용          
   서울시교통정보시스템 API 잘 동작하지 않아 데이터를 해킹하여 만듬

4. 꿈 이야기
  http://github.com/suminb/

[두번째 - 서버개발자가 되기전에 알았으면 좋았을 것들] 
한 서버 개발자의 취업 경험기(1년 8개월 되었음)
1. 컴퓨터 공학과 나왔지만 출판업에 있다가 서버 개발자가 됨
2. 뭘 모르는지 알아 보기
 - 메일링이나 블로그 확인
3. 책을 읽자
4. 취미/일상 코딩
 - 단어 바꾸기(정규식)
 - 크롤링 자동화 : Amazon의 신간정보 판매 순위
 - 길냥이 돕기(cat dad..)
5. 유닌그와 친해 지기
 - 관련 도구
 - 배포/운영
 - 기존 시스템 설정 파악
 - 다양한 명령어 익히기
6. 나와 맞는 회사
 - 내가 능력이 될까?
 - 들어간 회사가 너무 빡세면 어떻게 하지?
 - PyCon 과 IRC에서 알게된 네트워크를 통하여 취업을 하게 됨
 - 개인 프로젝트에서 얻은 교훈을 통하여 취업에 성공(Smart Study)
7. 정리
 - 뭘 모르는지 파악하기, 취미/일상에서 프로젝트 수행.....
 * 가장 중요한 것은 동기 부여하는 것

[세번째 - 한국에서 소프트웨어 카펜트리 1년] 
1. Software Carpentry
 - 좀더 생산적으로 연구를 수행하기 위해서 과학, 공학, 의학 분야의 지식근로자가 컴퓨터를 사용하게 한다.
2. 소프트웨어 카펜트리 해당
 - 과학자가 과학자를 가르침
 - 실습을 통한 2일 학습
3. 추진 결과
 - 생산성 10~20% 향상
4. 과학기술컴퓨팅
 - GDP 성장에 지식(Knowledge) 기여를 많이 하고 있음
5. 2일간의 워크샵을 4번에 걸쳐 진행
 - Translating software arpentry into Korean

[네번째 - JWT(Json Web Token] 김태웅
Grape란 App을 만들고 있음(연락처 App)

- 발표자 하는 이야기를 이해하기가 어려움(Django, JQuery 등등 주제는 많으나 접점이 보이지 않음)
- JWT을 통하여 기존의 인증 방식을 바꿀 수는 있으나
  base64라서 바로 복호화 가능하나 간단함
- pyjwt, django-jwt-auth, djangorestframework-jwt, jwt.io 참조

[다섯번째 - AllPairs python Lib] 김성준
소프트웨어 품질 개선을 위한 컨설팅 업무를 수행하고 있습니다.
Allpairs Lib은 단위테스트 케이스 만들어 주는 라이브러리(Test Combinations Generator)
Allpairs 로직은 발생 가능한 케이스를 효과적으로 유지하면서 개수는 효율적으로 줄여준다.
(이런 로직 없으면, 테스트 케이스는 무한하게 늘어 날 수 있다.)

[여섯번째 - IT 융합 트랜드가 나타나는 이유] - 송승호
개발을 하다가 마케팅 업무를 하고 있음
트랜트가 융합으로 가능 이유에 대하여 궁금해서 분석해 보았음
1. 변화하는 세상
 - 모바일 기기(휴대폰) 급속한 증가
 - 세상과 연결(Networking)된 장치가 증가(CCTV, Mobile Phone, ...)
 - Google 혹은 Twitter 어플을 통하여 개인 사용자 정보 취합 및 제공(Google Analytics)
   모이는 데이터가 많으니.. 데이터가 많아지고 컴퓨팅 환경도 클라우드로 바뀌고 있음

2. 메이블린 vs 에스티로더가 3D 프린팅 기술을 사용하는 MINK와 경쟁하고 있음
  - 고가 화장품이 저가 화장품(MINK) 경쟁하고 있음

3. [Gartner]2015년이 되면 65%의 IT 예산이 IT 부서가 아닌 CMO 조직(실행 조직)에서 나올 것이다.
  (IT 기술(정보)이 한곳에 집중에서 분산되는 추세가 있음)
  

[일곱번째 - 병렬 Python] 
1. parallel ssh tool
2. Distributed SSH 만들어 보자
  - 1000라인
  - Multiple thread
  - 간단한 DB 연동

[여덞번째 - 2008년 파이썬 시작 동기, Embeded system 부터 서버까지 하나의 프레임워크로서의 파이썬] 채문창
1. 전산과 졸업후 1991년 프로그램머 시작
  - 지훈현서 블로그 운영
2. 파이션 이용 동기
 - VIX API - Perl로 작성했는데 300짜리 코드를 만듬. 가동성 및 유지보수 좋고, 오픈소스인 파이썬 사용

3. MIPS, Cavium, ARM 등과 같은 Embeded 시스템에서 파이썬 프레임워크 적용


